{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOeZ3AmfEH/kEGxV7M83G4n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["import torch"],"metadata":{"id":"HucUjnklO284","executionInfo":{"status":"ok","timestamp":1683765596724,"user_tz":240,"elapsed":4024,"user":{"displayName":"Arunita Sarkar","userId":"18428709193733454795"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNImd71AOPPx"},"outputs":[],"source":["from IPython.core.displayhook import tokenize\n","# classification/regression problems\n","class CustomDataset:\n","  def __init__(self, data, targets, tokenizer):\n","    self.data = data\n","    self.targets = targets\n","    self.tokenizer = tokenizer\n","  \n","  def __len__(self):\n","    return len(self.data)\n","  \n","  def __getitem__(self, idx):\n","     text = self.data[idx]\n","     target = self.targets[idx]\n","     \n","     input_ids = tokenizer(text)\n","\n","     return {\n","      \"text\": torch.tensor(input_ids, dtype = torch.long),\n","      #\"attention_mask\": torch.tensor(attention_mask, dtype = torch.long),\n","      \"target\": torch.tensor(target, dtype = torch.long)\n","  }\n","      "]}]}